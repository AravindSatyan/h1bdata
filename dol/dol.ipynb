{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongodb='BINFOpJQiWmp1JIa'\n",
    "mongodb1='25w9gXhCYvSdNYho'\n",
    "\n",
    "import re\n",
    "import glob\n",
    "lst=glob.glob('**/*')\n",
    "lst\n",
    "for i in lst:\n",
    "    a=re.search('FY\\d*\\_?\\w+',''.join(i))\n",
    "    a=a.group(0)\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame is successfully dumped into PostgreSQL database.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, datetime as dt, glob\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "def pg_db_connection():\n",
    "        db_username = 'postgres'\n",
    "        db_password = 'your_password'\n",
    "        db_host = 'localhost'\n",
    "        db_port = '5432'\n",
    "        db_name = 'hariaravi'\n",
    "        # SQLAlchemy engine for PostgreSQL\n",
    "        engine = create_engine(f'postgresql+psycopg2://{db_username}:{db_password}@{db_host}:{db_port}/{db_name}')\n",
    "        return engine\n",
    "\n",
    "\n",
    "def main_dataset():\n",
    "        \n",
    "        a=glob.glob('**/*')\n",
    "        df=pd.DataFrame()\n",
    "        for i in range(len(a)):\n",
    "                k=a[i]\n",
    "                df_start=dt.datetime.now()\n",
    "                df=pd.read_excel(k)\n",
    "                df['link']=f'{k}'\n",
    "                u = df.select_dtypes(include=['datetime'])\n",
    "                df[u.columns] = u.fillna('None')\n",
    "                df=df.fillna('None')\n",
    "                df_end=dt.datetime.now()\n",
    "                print(f'df, done {k} {df.shape} time is: {df_end-df_start}')\n",
    "                sql_start=dt.datetime.now()\n",
    "                df.to_sql(f'table_is_{i}', engine, if_exists='replace', index=False)\n",
    "                sql_end=dt.datetime.now()\n",
    "                print(f'table_is_{i} also done and the time it took is {sql_end-sql_start}')\n",
    "\n",
    "        return df\n",
    "\n",
    "# df=main_dataset()\n",
    "print(\"DataFrame is successfully dumped into PostgreSQL database.\")\n",
    "# df = pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table is 0, the shape is: (120306, 97)\n",
      "time to query: 0:00:07.455208\n",
      "time to json: 0:00:03.032074\n"
     ]
    }
   ],
   "source": [
    "def pg_db_connection():\n",
    "        db_username = 'postgres'\n",
    "        db_password = 'your_password'\n",
    "        db_host = 'localhost'\n",
    "        db_port = '5432'\n",
    "        db_name = 'hariaravi'\n",
    "        # SQLAlchemy engine for PostgreSQL\n",
    "        engine = create_engine(f'postgresql+psycopg2://{db_username}:{db_password}@{db_host}:{db_port}/{db_name}')\n",
    "        return engine\n",
    "\n",
    "def get_full_data(engine):\n",
    "\n",
    "    with open (r'dol-h1b-data-final.json', mode='a', encoding = 'utf-8-sig') as file:\n",
    "        for i in range(20):\n",
    "            starttime_sql_query=dt.datetime.now()\n",
    "            query= f'select * from table_is_{i}'\n",
    "            temp_df = pd.read_sql(query, engine)\n",
    "            endtime_sql_query = dt.datetime.now()\n",
    "            temp_df.to_json(file, orient='records')\n",
    "            end_time_df_json= dt.datetime.now()\n",
    "            print(f'table is {i}, the shape is: {temp_df.shape}')\n",
    "            print(f'time to query: {endtime_sql_query-starttime_sql_query}')\n",
    "            print(f'time to json: {end_time_df_json - endtime_sql_query}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data has been inserted into the 'users' collection.\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import datetime as dt\n",
    "\n",
    "def load_sample_data(df):\n",
    "    mono_start=dt.datetime.now()\n",
    "    atlas_connection_string = \"mongodb+srv://aravindbedean:25w9gXhCYvSdNYho@cluster0.cbcz4vn.mongodb.net/?retryWrites=true&w=majority\"\n",
    "    client = MongoClient(atlas_connection_string)\n",
    "    db = client['sample_db']\n",
    "    collection = db['users']\n",
    "    collection.insert_many(df)\n",
    "    mongo_end=dt.datetime.now()\n",
    "    print(f'time to load to mongo is: {mongo_end-mono_start}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json, datetime as dt\n",
    "# query= 'select * from table_is_0'\n",
    "# temp_df = pd.read_sql(query, engine)\n",
    "\n",
    "# with open (r'/Users/hariaravi/PycharmProjects/finalGHfolder/Data_Engineering/Projects/h1b/dol-h1b-data-final.json', mode='a', encoding = 'utf-8-sig') as file:\n",
    "#     for i in range(20):\n",
    "#         # starttime_sql_query=dt.datetime.now()\n",
    "#         query= f'select * from table_is_{i}'\n",
    "#         temp_df = pd.read_sql(query, engine)\n",
    "#         # endtime_sql_query = dt.datetime.now()\n",
    "#         temp_df.to_json(file, lines=True, orient='records')\n",
    "#         # end_time_df_json= dt.datetime.now()\n",
    "#         # print(f'table is {i}, the shape is: {temp_df.shape}')\n",
    "#         # print(f'time to query: {endtime_sql_query-starttime_sql_query}')\n",
    "#         # print(f'time to json: {end_time_df_json - endtime_sql_query}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##libraries\n",
    "\n",
    "\n",
    "import pandas as pd, datetime as dt, glob, decimal, ijson, time as t, os, json, pyarrow\n",
    "from pytz import timezone\n",
    "from bson.decimal128 import Decimal128\n",
    "from sqlalchemy import create_engine\n",
    "from pymongo import MongoClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, datetime as dt, glob, decimal, ijson, time as t, os, json, pyarrow\n",
    "from pytz import timezone\n",
    "from bson.decimal128 import Decimal128\n",
    "from sqlalchemy import create_engine\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "def pg_db_connection():\n",
    "        db_username = 'postgres'\n",
    "        db_password = 'your_password'\n",
    "        db_host = 'localhost'\n",
    "        db_port = '5432'\n",
    "        db_name = 'hariaravi'\n",
    "        # SQLAlchemy engine for PostgreSQL\n",
    "        engine = create_engine(f'postgresql+psycopg2://{db_username}:{db_password}@{db_host}:{db_port}/{db_name}')\n",
    "        return engine\n",
    "\n",
    "def date_and_time():\n",
    "    format = \"%Y-%m-%d_%H-%M\"\n",
    "    now_utc = dt.datetime.now(timezone('EST'))\n",
    "    return now_utc.strftime(format)\n",
    "\n",
    "def get_full_data(engine,datetime):\n",
    "    data_file = f'final/dol-h1b-data-final-{datetime}.json'\n",
    "    with open (r'{}'.format(data_file), mode='a', encoding = 'utf-8-sig') as file:\n",
    "        for i in range(2):\n",
    "            starttime_sql_query=dt.datetime.now()\n",
    "            query= f'select * from table_is_{i}'\n",
    "            temp_df = pd.read_sql(query, engine)\n",
    "            endtime_sql_query = dt.datetime.now()\n",
    "            temp_df.to_json(file, orient='records')\n",
    "            end_time_df_json= dt.datetime.now()\n",
    "            print(f'table is {i}, the shape is: {temp_df.shape}')\n",
    "            print(f'time to query: {endtime_sql_query-starttime_sql_query}')\n",
    "            print(f'time to json: {end_time_df_json - endtime_sql_query}')\n",
    "        file_name = file.name\n",
    "        abs_path=os.path.abspath(file_name)\n",
    "        return abs_path     \n",
    "\n",
    "def convert_decimal128(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            obj[k] = convert_decimal128(v)\n",
    "    elif isinstance(obj, list):\n",
    "        obj = [convert_decimal128(v) for v in obj]\n",
    "    elif isinstance(obj, decimal.Decimal):\n",
    "        return Decimal128(str(obj))\n",
    "    return obj\n",
    "\n",
    "\n",
    "\n",
    "def load_data(df):\n",
    "    with open (r'{}'.format(df),mode = 'r', encoding = 'utf-8-sig') as file:\n",
    "        array_items = ijson.items(file, 'item')\n",
    "        temp_list=[]\n",
    "        count=0\n",
    "        for i in array_items:\n",
    "            atlas_connection_string = \"mongodb+srv://aravindbedean:25w9gXhCYvSdNYho@cluster0.cbcz4vn.mongodb.net/?retryWrites=true&w=majority\"\n",
    "            client = MongoClient(atlas_connection_string)\n",
    "            db = client['dol-h1b']\n",
    "            collection = db['datadump']\n",
    "            i=convert_decimal128(i)\n",
    "            collection.insert_one(i)\n",
    "        # temp_list.clear()\n",
    "        # print(len(temp_list))\n",
    " \n",
    "                \n",
    "def operation():\n",
    "    pg_db = pg_db_connection()\n",
    "    temp_variable=get_full_data(pg_db, date_and_time())\n",
    "    # final_end = load_data(temp_variable)\n",
    "    # final_end = load_data(r'/Users/hariaravi/PycharmProjects/dol-h1b/h1bdata/dol/final/dol-h1b-data-final-2024-02-24_16-57.json')\n",
    "    \n",
    "    print('All functions done')\n",
    "\n",
    "        \n",
    "# operation()\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-25 20:00:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Your timestamp in milliseconds\n",
    "timestamp_ms = \"1632614400000\"\n",
    "\n",
    "# Convert the string to an integer and then to seconds\n",
    "timestamp_s = int(timestamp_ms) / 1000.0\n",
    "\n",
    "# Convert the timestamp to a datetime object\n",
    "dt = datetime.fromtimestamp(timestamp_s)\n",
    "\n",
    "print(dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "with open (r'/Users/hariaravi/PycharmProjects/dol-h1b/h1bdata/dol/final/dol-h1b-data-final-2024-02-24_16-57.json', mode='r',encoding='utf-8-sig') as file:\n",
    "    a_file=ijson.items(file,\"item\")\n",
    "    temp_list=[]\n",
    "    for i,j in enumerate(a_file):\n",
    "        if len(temp_list) == 10: #1000000):\n",
    "            break\n",
    "        elif len(temp_list) < 10: #1000000):\n",
    "            temp_list.append(j)\n",
    "\n",
    "    print(len(temp_list))\n",
    "    mini_df=pd.DataFrame(temp_list)\n",
    "    # print(mini_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##self-serving-analytics-page 1\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def pg_db_connection():\n",
    "        db_username = 'postgres'\n",
    "        db_password = 'your_password'\n",
    "        db_host = 'localhost'\n",
    "        db_port = '5432'\n",
    "        db_name = 'hariaravi'\n",
    "        # SQLAlchemy engine for PostgreSQL\n",
    "        engine = create_engine(f'postgresql+psycopg2://{db_username}:{db_password}@{db_host}:{db_port}/{db_name}')\n",
    "        return engine\n",
    "\n",
    "\n",
    "df=pd.DataFrame()\n",
    "cit_y='cupertino'\n",
    "rol_e='data'\n",
    "for i in range(20):\n",
    "    engine = pg_db_connection()\n",
    "    temp_df = pd.read_sql(('select * from table_is_'+str(i)+' where  \\\"EMPLOYER_CITY\\\" ilike %(city)s and \\\"JOB_TITLE\\\" ilike %(role)s')\n",
    "                          , engine, params={\"city\": f'%{cit_y}%', \"role\":f'%{rol_e}%'})\n",
    "    temp_df['table'] = f'table_is_{i}'\n",
    "    df=pd.concat([df,temp_df])\n",
    "    print(f'table_is_{i} has the following shape {temp_df.shape}')\n",
    "\n",
    "df.to_excel(f'{cit_y}_{rol_e}.xlsx')\n",
    "print('Excel sheet is ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azure.storage.blob import BlobServiceClient\n",
    "from io import BytesIO\n",
    "import requests, re, pandas as pd, datetime as dt\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# def connection(file_name, data):\n",
    "#     acc_key = 'K6fR4BUxlGBJmX/mwn+5wUtKzwoDWwt7scCe/gvMZ2JIZ2hmrPsAw4ZNmmg/yaZaAcM14oyuTwDt+AStEjw/gQ=='\n",
    "#     acc_name= 'adlsfordol'\n",
    "#     acc_string='DefaultEndpointsProtocol=https;AccountName=adlsfordol;AccountKey=K6fR4BUxlGBJmX/mwn+5wUtKzwoDWwt7scCe/gvMZ2JIZ2hmrPsAw4ZNmmg/yaZaAcM14oyuTwDt+AStEjw/gQ==;EndpointSuffix=core.windows.net'\n",
    "#     conatiner_name = 'adls-container-dol-h1b/supportfiles'\n",
    "#     s_clinet = BlobServiceClient.from_connection_string(acc_string)\n",
    "#     blob_client = s_clinet.get_blob_client(container=conatiner_name,blob = file_name)\n",
    "#     blob_client.upload_blob(data, timeout = 1500, overwrite = True)\n",
    "\n",
    "def get_data():\n",
    "    url='https://www.dol.gov/agencies/eta/foreign-labor/performance'\n",
    "    r=requests.get(url)\n",
    "    soup=BeautifulSoup(r.content,'html.parser')\n",
    "    res=soup.select('table.cust_table')\n",
    "    final_links=[]\n",
    "    keywords=['Appendix', 'Worksites', 'Icert']\n",
    "    for i in res:\n",
    "        if i.get('summary') == 'LCA Programs (H-1B, H-1B1, E-3)':\n",
    "            i= i.find_all('a')\n",
    "            for j in i:\n",
    "                j=j.get('href')\n",
    "                excel_sheet = re.search('.xlsx',j)\n",
    "                key_search = re.search(\"|\".join(re.escape(word) for word in keywords), j)\n",
    "                if  excel_sheet is not None and key_search is None :\n",
    "                    final_links.append(j)\n",
    "    return final_links\n",
    "\n",
    "def uplaod_to_azure(final_links):\n",
    "    for i,j in enumerate(final_links):\n",
    "        if i!=4:\n",
    "            t1=dt.datetime.now()\n",
    "            sample = requests.get(f'https://www.dol.gov{j}')\n",
    "            sample = BytesIO(sample.content)\n",
    "            check = re.search(r'pdfs\\/(.*)',j)\n",
    "            checked = check.group().replace('pdfs/','')\n",
    "            # connection(checked,sample)\n",
    "            t2=dt.datetime.now()\n",
    "            print(f'time it took for content#: {i}, is: {t2-t1}')\n",
    "    return 'done'\n",
    "\n",
    "# print(uplaod_to_azure(get_data()))\n",
    "final_links=get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "\n",
    "for i in final_links:\n",
    "    sample = requests.get(f'https://www.dol.gov{i}')\n",
    "    excel_file = sample.content\n",
    "    # df=pd.read_excel(BytesIO(excel_file))\n",
    "    print(excel_file)\n",
    "    break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the big block to azure\n",
    "\n",
    "\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient, __version__\n",
    "import os\n",
    "\n",
    "# Your Azure Blob Storage connection string\n",
    "connect_str = 'your_connection_string'\n",
    "container_name = 'your_container_name'\n",
    "blob_name = 'your_append_blob_name'\n",
    "\n",
    "# Initialize the BlobServiceClient\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
    "container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "# Create the container if it does not exist\n",
    "container_client.create_container()\n",
    "\n",
    "# Get the blob client for the append blob\n",
    "blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "\n",
    "try:\n",
    "    # Create the append blob if it does not exist\n",
    "    blob_client.create_append_blob()\n",
    "except Exception as e:\n",
    "    print(f\"Blob already exists or another error occurred: {e}\")\n",
    "\n",
    "# Path to your large file\n",
    "file_path = 'path_to_your_large_file'\n",
    "\n",
    "# Open the file and append data in chunks\n",
    "with open(file_path, 'rb') as file:\n",
    "    chunk_size = 4 * 1024 * 1024  # 4MB chunk size\n",
    "    chunk = file.read(chunk_size)\n",
    "    while chunk:\n",
    "        blob_client.append_block(chunk)\n",
    "        chunk = file.read(chunk_size)\n",
    "\n",
    "print(f\"Finished uploading {file_path} to {blob_name} as an append blob.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
